{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Perceptron de Rosenblatt (1958) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "It is only after much hesitation that the writer has reconciled himself to the addition of the term \"neurodynamics\" to the list of such recent linguistic artifacts as \"cybernetics, \"bionics\",..., \"intelectronics\", and \"robotics\". It is hoped that by selecting a term which more clearly delimits our realm of interest and indicates its relationship to traditional academic disciplines, the underlying motivation of the perceptron program may be more successfully communicated. The term \"perceptron\", originally intended as a generic name for a variety of ***theoretical nerve nets***, has an unfortunate tendency to suggest a specific piece of hardware...\n",
    "\n",
    "For this writer, the perceptron program is not primarily concerned with the invention of devices for **\"artificial intelligence\"**, but rather with investigating the physical structures and neurodynamic principles which underlie \"natural intelligence\". A perceptron is first and foremost a brain model, not an invention for pattern recognition. As a brain model, its utility is in enabling us to determine the physical conditions for the emergence of various psychological properties.\n",
    "\n",
    "Prefacio de Frank Rossenblatt de su libro $\\text{Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms}$, Spartan Books, 1962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El objetivo inicial de Rosenblatt era hacer un modelo del cerebro humano para entenderlo mejor y no un mecanismo para el reconocimiento de patrones. \n",
    "\n",
    "Fueron otros los que usaron los principios propuestos por Rosenblatt y los aplicaron para resolver el problema de reconocimiento de patrones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El perceptron de Rosenblat es un algorítmo que resulve el problema de clasificación binaria. Es un tipo de clasificador lineal, es decir, solo puede clasificar problemas con datos que son linealmente separables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://miro.medium.com/max/870/1*gKFs7YU44vJFiS2rF3-bpg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El perceptrón por su forma solamente podría resolver datasets que son linealmente separables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorítmo de Aprendizaje del perceptron. Ajustando W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://miro.medium.com/max/516/1*PbJBdf-WxR0Dd0xHvEoh4A.png'> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(1, 0, 0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1, 0, 1)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1, 1, 0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1, 1, 1)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        data  target\n",
       "0  (1, 0, 0)       1\n",
       "1  (1, 0, 1)       1\n",
       "2  (1, 1, 0)       1\n",
       "3  (1, 1, 1)       0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conjunto_de_entrenamiento_df = pd.DataFrame([((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)], columns=['data', 'target'])\n",
    "conjunto_de_entrenamiento_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Los pesos son: \n",
      "[0, 0, 0]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.1, 0.0, 0.0]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 2\n",
      "\n",
      "Los pesos son: \n",
      "[0.2, 0.0, 0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 3\n",
      "\n",
      "Los pesos son: \n",
      "[0.30000000000000004, 0.1, 0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 0\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 3\n",
      "\n",
      "------------------------------------------------------------\n",
      "Los pesos son: \n",
      "[0.30000000000000004, 0.1, 0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.4, 0.1, 0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 2\n",
      "\n",
      "Los pesos son: \n",
      "[0.5, 0.1, 0.2]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 2\n",
      "\n",
      "Los pesos son: \n",
      "[0.5, 0.1, 0.2]\n",
      "El resultado es: 1\n",
      "Se esperaba: 0\n",
      "Entonces el error es: -1\n",
      "\n",
      "El contador de errores es: 3\n",
      "\n",
      "------------------------------------------------------------\n",
      "Los pesos son: \n",
      "[0.4, 0.0, 0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.5, 0.0, 0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.5, 0.0, 0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 2\n",
      "\n",
      "Los pesos son: \n",
      "[0.6, 0.1, 0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 0\n",
      "Entonces el error es: -1\n",
      "\n",
      "El contador de errores es: 3\n",
      "\n",
      "------------------------------------------------------------\n",
      "Los pesos son: \n",
      "[0.5, 0.0, 0.0]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.6, 0.0, 0.0]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.6, 0.0, 0.0]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.6, 0.0, 0.0]\n",
      "El resultado es: 1\n",
      "Se esperaba: 0\n",
      "Entonces el error es: -1\n",
      "\n",
      "El contador de errores es: 2\n",
      "\n",
      "------------------------------------------------------------\n",
      "Los pesos son: \n",
      "[0.5, -0.1, -0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.6, -0.1, -0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 2\n",
      "\n",
      "Los pesos son: \n",
      "[0.7, -0.1, 0.0]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 2\n",
      "\n",
      "Los pesos son: \n",
      "[0.7, -0.1, 0.0]\n",
      "El resultado es: 1\n",
      "Se esperaba: 0\n",
      "Entonces el error es: -1\n",
      "\n",
      "El contador de errores es: 3\n",
      "\n",
      "------------------------------------------------------------\n",
      "Los pesos son: \n",
      "[0.6, -0.2, -0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 0\n",
      "\n",
      "Los pesos son: \n",
      "[0.6, -0.2, -0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.7, -0.2, 0.0]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 2\n",
      "\n",
      "Los pesos son: \n",
      "[0.7999999999999999, -0.1, 0.0]\n",
      "El resultado es: 1\n",
      "Se esperaba: 0\n",
      "Entonces el error es: -1\n",
      "\n",
      "El contador de errores es: 3\n",
      "\n",
      "------------------------------------------------------------\n",
      "Los pesos son: \n",
      "[0.7, -0.2, -0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 0\n",
      "\n",
      "Los pesos son: \n",
      "[0.7, -0.2, -0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 0\n",
      "\n",
      "Los pesos son: \n",
      "[0.7, -0.2, -0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.7999999999999999, -0.1, -0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 0\n",
      "Entonces el error es: -1\n",
      "\n",
      "El contador de errores es: 2\n",
      "\n",
      "------------------------------------------------------------\n",
      "Los pesos son: \n",
      "[0.7, -0.2, -0.2]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 0\n",
      "\n",
      "Los pesos son: \n",
      "[0.7, -0.2, -0.2]\n",
      "El resultado es: 0\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 1\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.7999999999999999, -0.2, -0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "Los pesos son: \n",
      "[0.7999999999999999, -0.2, -0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 0\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 1\n",
      "\n",
      "------------------------------------------------------------\n",
      "Los pesos son: \n",
      "[0.7999999999999999, -0.2, -0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 0\n",
      "\n",
      "Los pesos son: \n",
      "[0.7999999999999999, -0.2, -0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 0\n",
      "\n",
      "Los pesos son: \n",
      "[0.7999999999999999, -0.2, -0.1]\n",
      "El resultado es: 1\n",
      "Se esperaba: 1\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 0\n",
      "\n",
      "Los pesos son: \n",
      "[0.7999999999999999, -0.2, -0.1]\n",
      "El resultado es: 0\n",
      "Se esperaba: 0\n",
      "Entonces el error es: 0\n",
      "\n",
      "El contador de errores es: 0\n",
      "\n",
      "\n",
      "Finalmente, el error es 0!\n"
     ]
    }
   ],
   "source": [
    "umbral = 0.5\n",
    "tasa_de_aprendizaje = 0.1\n",
    "pesos = [0, 0, 0]\n",
    "conjunto_de_entrenamiento = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]\n",
    "\n",
    "def producto_punto(valores, pesos):\n",
    "    return sum(valor * peso for valor, peso in zip(valores, pesos))\n",
    "\n",
    "while True:\n",
    "    print('-' * 60)\n",
    "    contador_de_errores = 0\n",
    "    for vector_de_entrada, salida_deseada in conjunto_de_entrenamiento:\n",
    "        print('Los pesos son: ')\n",
    "        print(pesos)\n",
    "        resultado = producto_punto(vector_de_entrada, pesos) > umbral\n",
    "        print('El resultado es: '+str(int(resultado)))\n",
    "        print('Se esperaba: '+str(salida_deseada))\n",
    "        error = salida_deseada - resultado\n",
    "        print('Entonces el error es: '+str(error))\n",
    "        if error != 0:\n",
    "            contador_de_errores += 1\n",
    "            for indice, valor in enumerate(vector_de_entrada):\n",
    "                pesos[indice] += tasa_de_aprendizaje * error * valor\n",
    "        print('\\nEl contador de errores es: '+str(contador_de_errores)+'\\n')\n",
    "        \n",
    "    if contador_de_errores == 0:\n",
    "        print(\"\\nFinalmente, el error es 0!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src= 'PerceptroTraining1.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src= 'PerceptroTraining2.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src= 'PerceptroTraining3.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src= 'PerceptroTraining4.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Redes Neuronales \n",
    "\n",
    "(Warren McCulloch y Walter Pitts ) (1943)\n",
    "\n",
    "\n",
    "\n",
    "### Neuronas\n",
    "\n",
    "Fuente de la mayoría de lo que sigue: https://victorzhou.com/blog/intro-to-neural-networks/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://i.stack.imgur.com/KUvpQ.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Las redes neuronales difieren de los perceptrones en que una transformación no-lineal diferenciable es aplicada en la salida. En las redes neuronales, para cada neurona se cumple lo siguiente:\n",
    "\n",
    "Si tenemos $x_i$ inputs para una neurona y $w_i$ pesos con $x_0 = 1$ y $w_0=b$, entonces el proceso de calculo de la señal de salida de la neurona es: \n",
    "\n",
    "$$f( \\sum_{i=o}^{n} x_i*w_i)$$\n",
    "\n",
    "La función f es llamada una función de activación y hay varias usadas en la literatura. Una común es la función sigmoide, que ya encontrabamos cuando tratabamos el tema de regresión logística. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recuerden que la función sigmoide está dada por: \n",
    "$$f(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "<img src='https://victorzhou.com/static/dd5a39500acbef371d8d791d2cd381e0/e334e/sigmoid.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ejemplo: \n",
    "Supongan que tenemos una red con los siguientes parametros para los pesos y el 'sesgo': \n",
    "\n",
    "$$w=[0,1]$$\n",
    "$$b = 4 $$\n",
    "\n",
    "Le damos a la red de una neurona una entrada como la que sigue: $ x = [2, 3]$. Usando la función sigmoide cual sería el resultado de pasar estos inputs por nuestra neurona. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Los elempentos de entrada multiplicada por los pesos resulta en: \n",
    "\n",
    "$$b+ w_1*x_1 + w_2*x_2 = 0*2+1*3+4 =7 $$\n",
    "\n",
    "Luego, debemos hacer la transformación no lineal:\n",
    "\n",
    "$$f(7) =\\frac{1}{1+e^{-7}} = 0.99$$\n",
    "\n",
    "El resultado para una neurona con estos pesos y ese vector de entrada es 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Más de una capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora supongamos que tenemos más de una neurona. Y supongamos que están organizadas en capas. La capa inicial o Input layer es la que interactua directamente con los vectores de entrada. \n",
    "\n",
    "Las capas siguientes se llaman capas ocultas o hidden layers y la última capa, es la capa de salida o Output Layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://victorzhou.com/77ed172fdef54ca1ffcfb0bba27ba334/network.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Siguiendo con el ejemplo anterior, asumamos que todas las neuronas tienen los mismos pesos $w=[0,1]$ y el sesgo $b=0$, y que los valores de entrada son igual que antes $x = [2, 3]$:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Siguiendo con el ejemplo anterior, asumamos que todas las neuronas tienen los mismos pesos $w=[0,1]$ y el sesgo $b=0$, y que los valores de entrada son igual que antes $x = [2, 3]$:\n",
    "\n",
    "\n",
    "$$h_1 =h_2  \n",
    "=f(w⋅x+b)\n",
    "=f((0∗2)+(1∗3)+0)\n",
    "=f(3)\n",
    "=0.9526$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$$ o_1 =f(w⋅[h_1 ,h_2 ]+b)$$ \n",
    "\n",
    "$$=f((0∗h_1)+(1∗h_2)+0)=f(0.9526) =  0.7216 $$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La salida de la red es 0.7216. Siempre que tengan una red neuronal el proceso de propagación de información de entrada a la información de salida se realiza de la misma manera. Este proceso se llama ***feedforward***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entrenando una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lo primero que debemos hacer es comparar el resultado de la predicción de la red neuronal con el valor real que queríamos aprender. Como vimos con el perceptron el proceso de ajuste de los pesos se realiza de manera iterativa tomando cada uno de las observaciones en el dataset y el proceso termina cuando el error en cada una de las observaciones es 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Función de Error o \"Perdida\": \n",
    "La función error a usar depende del problema, si es regresión o clasificación. En regresión, en el caso de regresión podemos usar el error cuadrado promedio: \n",
    "\n",
    "$$ MSE  = \\frac{1}{n} \\sum_{i_0}^n (y_{real}-y_{pred})^2 $$\n",
    "\n",
    "donde $y_{pred}$ es el resultado de pasar datos de entrada $x$ a traves de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "\n",
    "print(mse_loss(y_true, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objetivo: Minimizar el error o perdida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para lograr cambiar la predicción y por ende la pérdida, podemos cambiar los pesos de la red. ¿Cómo se relaciona el error de predicción y los pesos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://victorzhou.com/27cf280166d7159c0465a58c68f99b39/network3.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entonces la función perdida es una función de todos los pesos y sesgos de cada neurona en el modelo.\n",
    "\n",
    "$$L(w_1, w_2, w_3, w_4, w_5, w_6, b_1, b_2, b_3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Queremos enteder como la función perdidad cambia al cambiar los pesos, por ejemplo $w_1$, para eso podríamos calcular $\\frac{\\partial L}{\\partial w_1}$.\n",
    "\n",
    "Usando la regla de la cadena vemos que $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El cambio en $w_1$ afecta la salida de la hidden unit $h_1$. Entonces podemos descomponer el último término aún mas: \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y_{pred}} *\\frac{\\partial y_{pred}}{\\partial h_1}* \\frac{\\partial h_1}{\\partial w_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Revisemos solo el último paso: \n",
    "\n",
    "$$h_1 =f(w_1*x_1 +w_2 *x_2 +b_1 )$$\n",
    "\n",
    "donde $f$ es nuestra función de activación, supongamos por ahora que es la función sigmoide. Y debemos diferenciar con respecto a $w_1$, vemos que el el resultado sería \n",
    "\n",
    "$$ \\frac{ \\partial h_1}{\\partial w_1}= x_1 f'(w_1*x_1 +w_2 *x_2 +b_1 )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La función $f$ la hemos asumido como $f(x) = \\frac{1}{1+e^{-x}}$. \n",
    "\n",
    "Entonces $$f'(x) = f(x)*(1-f(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Este proceso se llama error backpropagation o propagación del error hacia atras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vamos a usar una regla de actualización para cambiar los pesos y vamos a usar la propogación de errores para guiar la actualización de los pesos. \n",
    "\n",
    "Este algorítmo se basa en cambiar los pesos, digamos $w_1$ por: \n",
    "\n",
    "$$ w_1 \\leftarrow w_1 - \\eta \\frac{\\partial L}{\\partial w_1} $$\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Si $ \\frac{\\partial L}{\\partial w_1} $ es positivo, entonces $w_1$ va a disminuir y el error $L$ va a disminuir.\n",
    "* Si $ \\frac{\\partial L}{\\partial w_1} $ es negativo, entonces $w_1$ va a aumentar y el error $L$ va a disminuir.\n",
    "\n",
    "En cualquier caso estamos afectando el peso $w_1$ y con ello disminuyendo el error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Elegimimos una observación al azar y calculamos la predicción y el error\n",
    "2. Calculamos las derivadas parciales con respecto a los pesos y sesgos.\n",
    "3. Actualizamos los pesos\n",
    "4. De vuelta al paso 1. \n",
    "\n",
    "Repetimos esto hasta que estemos por debajo de un umbral aceptable de errores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.550\n",
      "Epoch 10 loss: 0.143\n",
      "Epoch 20 loss: 0.104\n",
      "Epoch 30 loss: 0.077\n",
      "Epoch 40 loss: 0.060\n",
      "Epoch 50 loss: 0.048\n",
      "Epoch 60 loss: 0.040\n",
      "Epoch 70 loss: 0.035\n",
      "Epoch 80 loss: 0.030\n",
      "Epoch 90 loss: 0.027\n",
      "Epoch 100 loss: 0.024\n",
      "Epoch 110 loss: 0.022\n",
      "Epoch 120 loss: 0.020\n",
      "Epoch 130 loss: 0.019\n",
      "Epoch 140 loss: 0.017\n",
      "Epoch 150 loss: 0.016\n",
      "Epoch 160 loss: 0.015\n",
      "Epoch 170 loss: 0.014\n",
      "Epoch 180 loss: 0.013\n",
      "Epoch 190 loss: 0.013\n",
      "Epoch 200 loss: 0.012\n",
      "Epoch 210 loss: 0.011\n",
      "Epoch 220 loss: 0.011\n",
      "Epoch 230 loss: 0.010\n",
      "Epoch 240 loss: 0.010\n",
      "Epoch 250 loss: 0.009\n",
      "Epoch 260 loss: 0.009\n",
      "Epoch 270 loss: 0.009\n",
      "Epoch 280 loss: 0.008\n",
      "Epoch 290 loss: 0.008\n",
      "Epoch 300 loss: 0.008\n",
      "Epoch 310 loss: 0.008\n",
      "Epoch 320 loss: 0.007\n",
      "Epoch 330 loss: 0.007\n",
      "Epoch 340 loss: 0.007\n",
      "Epoch 350 loss: 0.007\n",
      "Epoch 360 loss: 0.006\n",
      "Epoch 370 loss: 0.006\n",
      "Epoch 380 loss: 0.006\n",
      "Epoch 390 loss: 0.006\n",
      "Epoch 400 loss: 0.006\n",
      "Epoch 410 loss: 0.006\n",
      "Epoch 420 loss: 0.006\n",
      "Epoch 430 loss: 0.005\n",
      "Epoch 440 loss: 0.005\n",
      "Epoch 450 loss: 0.005\n",
      "Epoch 460 loss: 0.005\n",
      "Epoch 470 loss: 0.005\n",
      "Epoch 480 loss: 0.005\n",
      "Epoch 490 loss: 0.005\n",
      "Epoch 500 loss: 0.005\n",
      "Epoch 510 loss: 0.004\n",
      "Epoch 520 loss: 0.004\n",
      "Epoch 530 loss: 0.004\n",
      "Epoch 540 loss: 0.004\n",
      "Epoch 550 loss: 0.004\n",
      "Epoch 560 loss: 0.004\n",
      "Epoch 570 loss: 0.004\n",
      "Epoch 580 loss: 0.004\n",
      "Epoch 590 loss: 0.004\n",
      "Epoch 600 loss: 0.004\n",
      "Epoch 610 loss: 0.004\n",
      "Epoch 620 loss: 0.004\n",
      "Epoch 630 loss: 0.004\n",
      "Epoch 640 loss: 0.004\n",
      "Epoch 650 loss: 0.003\n",
      "Epoch 660 loss: 0.003\n",
      "Epoch 670 loss: 0.003\n",
      "Epoch 680 loss: 0.003\n",
      "Epoch 690 loss: 0.003\n",
      "Epoch 700 loss: 0.003\n",
      "Epoch 710 loss: 0.003\n",
      "Epoch 720 loss: 0.003\n",
      "Epoch 730 loss: 0.003\n",
      "Epoch 740 loss: 0.003\n",
      "Epoch 750 loss: 0.003\n",
      "Epoch 760 loss: 0.003\n",
      "Epoch 770 loss: 0.003\n",
      "Epoch 780 loss: 0.003\n",
      "Epoch 790 loss: 0.003\n",
      "Epoch 800 loss: 0.003\n",
      "Epoch 810 loss: 0.003\n",
      "Epoch 820 loss: 0.003\n",
      "Epoch 830 loss: 0.003\n",
      "Epoch 840 loss: 0.003\n",
      "Epoch 850 loss: 0.003\n",
      "Epoch 860 loss: 0.003\n",
      "Epoch 870 loss: 0.003\n",
      "Epoch 880 loss: 0.003\n",
      "Epoch 890 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n",
      "Epoch 910 loss: 0.002\n",
      "Epoch 920 loss: 0.002\n",
      "Epoch 930 loss: 0.002\n",
      "Epoch 940 loss: 0.002\n",
      "Epoch 950 loss: 0.002\n",
      "Epoch 960 loss: 0.002\n",
      "Epoch 970 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n",
      "Epoch 990 loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "  fx = sigmoid(x)\n",
    "  return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "\n",
    "  *** DISCLAIMER ***:\n",
    "  The code below is intended to be simple and educational, NOT optimal.\n",
    "  Real neural net code looks nothing like this. DO NOT use this code.\n",
    "  Instead, read/run it to understand how this specific network works.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.1\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss at the end of each epoch\n",
    "      if epoch % 10 == 0:\n",
    "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "        loss = mse_loss(all_y_trues, y_preds)\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "        losses.append(loss)\n",
    "    return losses\n",
    "\n",
    "# Define dataset\n",
    "data = np.array([\n",
    "  [-2, -1],  # Alice\n",
    "  [25, 6],   # Bob\n",
    "  [17, 4],   # Charlie\n",
    "  [-15, -6], # Diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Alice\n",
    "  0, # Bob\n",
    "  0, # Charlie\n",
    "  1, # Diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "losses = network.train(data, all_y_trues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.plot(range(1, 101), losses)\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[![TRAINING MLP VIDEO]()](https://www.youtube.com/embed/nrnxZVEHZCo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Otras funciones de Activación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### ReLU : Rectified Linear Unit\n",
    "\n",
    "$f(x) = 0$ si $x<0$ \n",
    "\n",
    "$f(x)=x$ si $x>=0$\n",
    "\n",
    "#### Softmax: \n",
    "Se emplea para \"comprimir\" un vector K-dimensional, $ \\mathbf {z}  $, de valores reales arbitrarios en un vector K-dimensional,$ \\sigma (\\mathbf {z} )$, de valores reales en el rango [0, 1].\n",
    "\n",
    "$f(x) = e^x/\\sum_i e^{x_i}$\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
